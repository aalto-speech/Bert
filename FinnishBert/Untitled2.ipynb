{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import model_1\n",
    "import optimization\n",
    "import tensorflow as tf\n",
    "\n",
    "flags = tf.flags\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_bool(\"use_fp16\",False, \"Whether to use fp32 or fp16 arithmetic on GPU.\")\n",
    "flags.DEFINE_bool(\"do_train\", True, \"Whether to run training.\")\n",
    "flags.DEFINE_bool(\"amp\", False, \"Whether to enable AMP ops.\")\n",
    "\n",
    "class _LogSessionRunHook(tf.train.SessionRunHook):\n",
    "  def __init__(self, global_batch_size, display_every=1, hvd_rank=-1):\n",
    "    self.global_batch_size = global_batch_size\n",
    "    self.display_every = display_every\n",
    "    self.hvd_rank = hvd_rank\n",
    "  def after_create_session(self, session, coord):\n",
    "    if FLAGS.use_fp16 or FLAGS.amp:\n",
    "      print('  Step samples/sec   MLM Loss  NSP Loss  Loss  Learning-rate  Loss-scaler')\n",
    "    else:\n",
    "      print('  Step samples/sec   MLM Loss  NSP Loss  Loss  Learning-rate')\n",
    "    self.elapsed_secs = 0.\n",
    "    self.count = 0\n",
    "  def before_run(self, run_context):\n",
    "    self.t0 = time.time()\n",
    "    if FLAGS.use_fp16 or FLAGS.amp:\n",
    "      return tf.train.SessionRunArgs(\n",
    "          fetches=['step_update:0', 'total_loss:0',\n",
    "                   'learning_rate:0','mlm_loss:0', 'loss_scale:0'])\n",
    "    else:\n",
    "      return tf.train.SessionRunArgs(\n",
    "          fetches=['step_update:0', 'total_loss:0',\n",
    "                   'learning_rate:0','mlm_loss:0'])\n",
    "  def after_run(self, run_context, run_values):\n",
    "    self.elapsed_secs += time.time() - self.t0\n",
    "    self.count += 1\n",
    "    if FLAGS.use_fp16 or FLAGS.amp:\n",
    "      global_step, total_loss, lr, mlm_loss, loss_scaler = run_values.results\n",
    "    else:\n",
    "      global_step, total_loss, lr, mlm_loss = run_values.results\n",
    "    print_step = global_step + 1 # One-based index for printing.\n",
    "    if print_step == 1 or print_step % self.display_every == 0:\n",
    "        dt = self.elapsed_secs / self.count\n",
    "        img_per_sec = self.global_batch_size / dt\n",
    "        if self.hvd_rank >= 0:\n",
    "          if FLAGS.use_fp16 or FLAGS.amp:\n",
    "            print('%2d :: %6i %11.1f %10.4e %6.3f     %6.4e  %6.4e' %\n",
    "                  (self.hvd_rank, print_step, img_per_sec, mlm_loss, total_loss, lr, loss_scaler))\n",
    "          else:\n",
    "            print('%2d :: %6i %11.1f %10.4e %6.3f     %6.4e' %\n",
    "                  (self.hvd_rank, print_step, img_per_sec, mlm_loss,total_loss, lr))\n",
    "        else:\n",
    "          if FLAGS.use_fp16 or FLAGS.amp:\n",
    "            print('%6i %11.1f %10.4e %6.3f     %6.4e  %6.4e' %\n",
    "                  (print_step, img_per_sec, mlm_loss, total_loss, lr, loss_scaler))\n",
    "          else:\n",
    "            print('%6i %11.1f %10.4e  %6.3f     %6.4e' %\n",
    "                  (print_step, img_per_sec, mlm_loss, total_loss, lr))\n",
    "        self.elapsed_secs = 0.\n",
    "        self.count = 0\n",
    "\n",
    "\n",
    "def model_fn_builder(bert_config, init_checkpoint, learning_rate,\n",
    "                     num_train_steps, num_warmup_steps, use_tpu,\n",
    "                     use_one_hot_embeddings, hvd=None):\n",
    "  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
    "\n",
    "  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
    "    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
    "\n",
    "    tf.logging.info(\"*** Features ***\")\n",
    "    for name in sorted(features.keys()):\n",
    "      tf.logging.info(\"  name = %s, shape = %s\" % (name, features[name].shape))\n",
    "\n",
    "    input_ids = features[\"input_ids\"]\n",
    "    input_mask = features[\"input_mask\"]\n",
    "    #segment_ids = features[\"segment_ids\"]\n",
    "    masked_lm_positions = features[\"masked_lm_positions\"]\n",
    "    masked_lm_ids = features[\"masked_lm_ids\"]\n",
    "    masked_lm_weights = features[\"masked_lm_weights\"]\n",
    "\n",
    "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "    model = model_1.BertModel(\n",
    "        config=bert_config,\n",
    "        is_training=is_training,\n",
    "        input_ids=input_ids,\n",
    "        input_mask=input_mask,\n",
    "        #token_type_ids=segment_ids,\n",
    "        use_one_hot_embeddings=use_one_hot_embeddings,\n",
    "        compute_type=tf.float16)\n",
    "\n",
    "    (masked_lm_loss,\n",
    "     masked_lm_example_loss, masked_lm_log_probs) = get_masked_lm_output(\n",
    "         bert_config, model.get_sequence_output(), model.get_embedding_table(),\n",
    "         masked_lm_positions, masked_lm_ids, masked_lm_weights)\n",
    "\n",
    "\n",
    "    masked_lm_loss = tf.identity(masked_lm_loss, name=\"mlm_loss\")\n",
    "    total_loss = masked_lm_loss\n",
    "    total_loss = tf.identity(total_loss, name='total_loss')\n",
    "\n",
    "    tvars = tf.trainable_variables()\n",
    "\n",
    "    initialized_variable_names = {}\n",
    "    scaffold_fn = None\n",
    "    if init_checkpoint and (hvd is None or hvd.rank() == 0):\n",
    "      (assignment_map, initialized_variable_names\n",
    "      ) = model_1.get_assignment_map_from_checkpoint(tvars, init_checkpoint)\n",
    "      if use_tpu:\n",
    "\n",
    "        def tpu_scaffold():\n",
    "          tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
    "          return tf.train.Scaffold()\n",
    "\n",
    "        scaffold_fn = tpu_scaffold\n",
    "      else:\n",
    "        tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
    "\n",
    "    tf.logging.info(\"**** Trainable Variables ****\")\n",
    "    for var in tvars:\n",
    "      init_string = \"\"\n",
    "      if var.name in initialized_variable_names:\n",
    "        init_string = \", *INIT_FROM_CKPT*\"\n",
    "      tf.logging.info(\"  %d :: name = %s, shape = %s%s\", 0 if hvd is None else hvd.rank(), var.name, var.shape,\n",
    "                      init_string)\n",
    "\n",
    "    output_spec = None\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "      train_op = optimization.create_optimizer(\n",
    "          total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu,\n",
    "          hvd, FLAGS.use_fp16, FLAGS.amp)\n",
    "\n",
    "      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
    "          mode=mode,\n",
    "          loss=total_loss,\n",
    "          train_op=train_op,\n",
    "          scaffold_fn=scaffold_fn)\n",
    "    elif mode == tf.estimator.ModeKeys.EVAL:\n",
    "\n",
    "      def metric_fn(masked_lm_example_loss, masked_lm_log_probs, masked_lm_ids,\n",
    "                    masked_lm_weights):\n",
    "        \"\"\"Computes the loss and accuracy of the model.\"\"\"\n",
    "        masked_lm_log_probs = tf.reshape(masked_lm_log_probs,\n",
    "                                         [-1, masked_lm_log_probs.shape[-1]])\n",
    "        masked_lm_predictions = tf.argmax(\n",
    "            masked_lm_log_probs, axis=-1, output_type=tf.int32)\n",
    "        masked_lm_example_loss = tf.reshape(masked_lm_example_loss, [-1])\n",
    "        masked_lm_ids = tf.reshape(masked_lm_ids, [-1])\n",
    "        masked_lm_weights = tf.reshape(masked_lm_weights, [-1])\n",
    "        masked_lm_accuracy = tf.metrics.accuracy(\n",
    "            labels=masked_lm_ids,\n",
    "            predictions=masked_lm_predictions,\n",
    "            weights=masked_lm_weights)\n",
    "        masked_lm_mean_loss = tf.metrics.mean(\n",
    "            values=masked_lm_example_loss, weights=masked_lm_weights)\n",
    "\n",
    "      \n",
    "\n",
    "        return {\n",
    "            \"masked_lm_accuracy\": masked_lm_accuracy,\n",
    "            \"masked_lm_loss\": masked_lm_mean_loss\n",
    "            \n",
    "        }\n",
    "\n",
    "      eval_metrics = (metric_fn, [\n",
    "          masked_lm_example_loss, masked_lm_log_probs, masked_lm_ids,\n",
    "          masked_lm_weights\n",
    "      ])\n",
    "      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
    "          mode=mode,\n",
    "          loss=total_loss,\n",
    "          eval_metrics=eval_metrics,\n",
    "          scaffold_fn=scaffold_fn)\n",
    "    else:\n",
    "      raise ValueError(\"Only TRAIN and EVAL modes are supported: %s\" % (mode))\n",
    "\n",
    "    return output_spec\n",
    "\n",
    "  return model_fn\n",
    "\n",
    "def get_masked_lm_output(bert_config, input_tensor, output_weights, positions,\n",
    "                         label_ids, label_weights):\n",
    "  \"\"\"Get loss and log probs for the masked LM.\"\"\"\n",
    "  input_tensor = gather_indexes(input_tensor, positions)\n",
    "\n",
    "  with tf.variable_scope(\"cls/predictions\"):\n",
    "    # We apply one more non-linear transformation before the output layer.\n",
    "    # This matrix is not used after pre-training.\n",
    "    with tf.variable_scope(\"transform\"):\n",
    "      input_tensor = tf.layers.dense(\n",
    "          input_tensor,\n",
    "          units=bert_config.hidden_size,\n",
    "          activation=model_1.get_activation(bert_config.hidden_act),\n",
    "          kernel_initializer=model_1.create_initializer(\n",
    "              bert_config.initializer_range))\n",
    "      input_tensor = model_1.layer_norm(input_tensor)\n",
    "\n",
    "    # The output weights are the same as the input embeddings, but there is\n",
    "    # an output-only bias for each token.\n",
    "    output_bias = tf.get_variable(\n",
    "        \"output_bias\",\n",
    "        shape=[bert_config.vocab_size],\n",
    "        initializer=tf.zeros_initializer())\n",
    "    logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n",
    "    logits = tf.nn.bias_add(logits, output_bias)\n",
    "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "\n",
    "    label_ids = tf.reshape(label_ids, [-1])\n",
    "    label_weights = tf.reshape(label_weights, [-1])\n",
    "\n",
    "    one_hot_labels = tf.one_hot(\n",
    "        label_ids, depth=bert_config.vocab_size, dtype=tf.float32)\n",
    "\n",
    "    # The `positions` tensor might be zero-padded (if the sequence is too\n",
    "    # short to have the maximum number of predictions). The `label_weights`\n",
    "    # tensor has a value of 1.0 for every real prediction and 0.0 for the\n",
    "    # padding predictions.\n",
    "    per_example_loss = -tf.reduce_sum(log_probs * one_hot_labels, axis=[-1])\n",
    "    numerator = tf.reduce_sum(label_weights * per_example_loss)\n",
    "    denominator = tf.reduce_sum(label_weights) + 1e-5\n",
    "    loss = numerator / denominator\n",
    "\n",
    "  return (loss, per_example_loss, log_probs)\n",
    "\n",
    "def gather_indexes(sequence_tensor, positions):\n",
    "  \"\"\"Gathers the vectors at the specific positions over a minibatch.\"\"\"\n",
    "  sequence_shape = model_1.get_shape_list(sequence_tensor, expected_rank=3)\n",
    "  batch_size = sequence_shape[0]\n",
    "  seq_length = sequence_shape[1]\n",
    "  width = sequence_shape[2]\n",
    "\n",
    "  flat_offsets = tf.reshape(\n",
    "      tf.range(0, batch_size, dtype=tf.int32) * seq_length, [-1, 1])\n",
    "  flat_positions = tf.reshape(positions + flat_offsets, [-1])\n",
    "  flat_sequence_tensor = tf.reshape(sequence_tensor,\n",
    "                                    [batch_size * seq_length, width])\n",
    "  output_tensor = tf.gather(flat_sequence_tensor, flat_positions)\n",
    "  return output_tensor\n",
    "\n",
    "def input_fn_builder(input_files,\n",
    "                     max_seq_length,\n",
    "                     max_predictions_per_seq,\n",
    "                     is_training,\n",
    "                     num_cpu_threads=4,\n",
    "                     hvd=None):\n",
    "  \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n",
    "\n",
    "  def input_fn(params):\n",
    "    \"\"\"The actual input function.\"\"\"\n",
    "    batch_size = params[\"batch_size\"]\n",
    "\n",
    "    name_to_features = {\n",
    "        \"input_ids\":\n",
    "            tf.FixedLenFeature([max_seq_length], tf.int64),\n",
    "        \"input_mask\":\n",
    "            tf.FixedLenFeature([max_seq_length], tf.int64),\n",
    "        #\"segment_ids\":\n",
    "         #   tf.FixedLenFeature([max_seq_length], tf.int64),\n",
    "        \"masked_lm_positions\":\n",
    "            tf.FixedLenFeature([max_predictions_per_seq], tf.int64),\n",
    "        \"masked_lm_ids\":\n",
    "            tf.FixedLenFeature([max_predictions_per_seq], tf.int64),\n",
    "        \"masked_lm_weights\":\n",
    "            tf.FixedLenFeature([max_predictions_per_seq], tf.float32),\n",
    "    }\n",
    "\n",
    "    # For training, we want a lot of parallel reading and shuffling.\n",
    "    # For eval, we want no shuffling and parallel reading doesn't matter.\n",
    "    if is_training:\n",
    "      d = tf.data.Dataset.from_tensor_slices(tf.constant(input_files))\n",
    "      if hvd is not None: d = d.shard(hvd.size(), hvd.rank())\n",
    "      d = d.repeat()\n",
    "      d = d.shuffle(buffer_size=len(input_files))\n",
    "\n",
    "      # `cycle_length` is the number of parallel files that get read.\n",
    "      cycle_length = min(num_cpu_threads, len(input_files))\n",
    "\n",
    "      # `sloppy` mode means that the interleaving is not exact. This adds\n",
    "      # even more randomness to the training pipeline.\n",
    "      d = d.apply(\n",
    "          tf.contrib.data.parallel_interleave(\n",
    "              tf.data.TFRecordDataset,\n",
    "              sloppy=is_training,\n",
    "              cycle_length=cycle_length))\n",
    "      d = d.shuffle(buffer_size=100)\n",
    "    else:\n",
    "      d = tf.data.TFRecordDataset(input_files)\n",
    "      # Since we evaluate for a fixed number of steps we don't want to encounter\n",
    "      # out-of-range exceptions.\n",
    "      d = d.repeat()\n",
    "\n",
    "    # We must `drop_remainder` on training because the TPU requires fixed\n",
    "    # size dimensions. For eval, we assume we are evaluating on the CPU or GPU\n",
    "    # and we *don't* want to drop the remainder, otherwise we wont cover\n",
    "    # every sample.\n",
    "    d = d.apply(\n",
    "        tf.contrib.data.map_and_batch(\n",
    "            lambda record: _decode_record(record, name_to_features),\n",
    "            batch_size=batch_size,\n",
    "            num_parallel_batches=num_cpu_threads,\n",
    "            drop_remainder=False))#Default was True\n",
    "    return d\n",
    "\n",
    "  return input_fn\n",
    "\n",
    "def _decode_record(record, name_to_features):\n",
    "  \"\"\"Decodes a record to a TensorFlow example.\"\"\"\n",
    "  example = tf.parse_single_example(record, name_to_features)\n",
    "\n",
    "  # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n",
    "  # So cast all int64 to int32.\n",
    "  #for name in list(example.keys()):\n",
    "   # t = example[name]\n",
    "    #if t.dtype == tf.int64:\n",
    "     # t = tf.to_int32(t)\n",
    "    #example[name] = t\n",
    "\n",
    "  return example\n",
    "\n",
    "\n",
    "\n",
    "def main(_):\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    bert_config = model_1.BertConfig.from_json_file('bert_config.json')\n",
    "    input_files = []\n",
    "    input_files.extend(tf.gfile.Glob('kielipankki_clean_train_300_final.tfrecord'))\n",
    "    tf.logging.info(\"*** Input Files ***\")\n",
    "    for input_file in input_files:\n",
    "        tf.logging.info(\"  %s\" % input_file)\n",
    "    \n",
    "    \n",
    "    config = tf.ConfigProto()\n",
    "    '''\n",
    "    The TensorFlow/XLA JIT compiler compiles and runs parts of TensorFlow graphs via XLA. The benefit of this over the standard TensorFlow implementation is that XLA can fuse multiple operators (kernel fusion) into a small number of compiled kernels. Fusing operators can reduce memory bandwidth requirements and improve performance compared to executing operators one-at-a-time, as the TensorFlow executor does.'''\n",
    "    tpu_cluster_resolver = None\n",
    "    config.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1\n",
    "    is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2\n",
    "    run_config = tf.contrib.tpu.RunConfig(\n",
    "        cluster=tpu_cluster_resolver,\n",
    "        master=None,\n",
    "        model_dir='outputckpoints',\n",
    "        session_config=config,\n",
    "        save_checkpoints_steps=1000,\n",
    "        tpu_config=tf.contrib.tpu.TPUConfig(\n",
    "            iterations_per_loop=1000,\n",
    "            num_shards=8,\n",
    "            per_host_input_for_training=is_per_host),\n",
    "        # This variable controls how often estimator reports examples/sec.\n",
    "        # Default value is every 100 steps.\n",
    "        # When --report_loss is True, we set to very large value to prevent\n",
    "        # default info reporting from estimator.\n",
    "        # Ideally we should set it to None, but that does not work.\n",
    "        log_step_count_steps=10000)\n",
    "    model_fn = model_fn_builder(\n",
    "        bert_config=bert_config,\n",
    "        init_checkpoint=None,\n",
    "        learning_rate=5e-5,\n",
    "        num_train_steps=100000,\n",
    "        num_warmup_steps=10000,\n",
    "        use_tpu=False,\n",
    "        use_one_hot_embeddings=False,\n",
    "        hvd=None)\n",
    "    training_hooks = []\n",
    "    training_hooks.append(_LogSessionRunHook(32,1,-1))\n",
    "    estimator = tf.contrib.tpu.TPUEstimator(\n",
    "        use_tpu=False,\n",
    "        model_fn=model_fn,\n",
    "        config=run_config,\n",
    "        train_batch_size=12,\n",
    "        eval_batch_size=8)\n",
    "    if FLAGS.do_train:\n",
    "        tf.logging.info(\"***** Running training *****\")\n",
    "        tf.logging.info(\"  Batch size = %d\",12)\n",
    "        train_input_fn = input_fn_builder(\n",
    "            input_files=input_files,\n",
    "            max_seq_length=300,\n",
    "            max_predictions_per_seq=40,\n",
    "            is_training=True,\n",
    "            hvd=None )\n",
    "        estimator.train(input_fn=train_input_fn, hooks=training_hooks, max_steps=100000)\n",
    "    \n",
    "    tf.logging.info(\"***** Running evaluation *****\")\n",
    "    tf.logging.info(\"  Batch size = %d\", 8)\n",
    "\n",
    "    eval_input_fn = input_fn_builder(\n",
    "        input_files=input_files,\n",
    "        max_seq_length=300,\n",
    "        max_predictions_per_seq=40,\n",
    "        is_training=False)\n",
    "\n",
    "    result = estimator.evaluate(\n",
    "        input_fn=eval_input_fn, steps=100)\n",
    "\n",
    "    output_eval_file = os.path.join('outputckpoints', \"eval_results.txt\")\n",
    "    with tf.gfile.GFile(output_eval_file, \"w\") as writer:\n",
    "      tf.logging.info(\"***** Eval results *****\")\n",
    "      for key in sorted(result.keys()):\n",
    "        tf.logging.info(\"  %s = %s\", key, str(result[key]))\n",
    "        writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tf.app.run()\n",
    "                          \n",
    "    \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
